{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 6390: Assignment 5\n",
    "## Due October 24th\n",
    "### By: Nicholas Lutrzykowski \n",
    "The goal of this assignment is to use SVM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements \n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and setup data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes: 30\n",
      "Number of points: 39644\n"
     ]
    }
   ],
   "source": [
    "data_orig = np.genfromtxt('OnlineNewsPopularity.csv', delimiter=',')\n",
    "\n",
    "data = np.concatenate((data_orig[1:,2:4], data_orig[1:, 7:13], data_orig[1:, 39:61]), axis=1)\n",
    "\n",
    "num_points = data.shape[0]\n",
    "num_attributes = data.shape[1]\n",
    "print(\"Number of attributes:\", num_attributes)\n",
    "print(\"Number of points:\", num_points)\n",
    "\n",
    "# Read in the titles \n",
    "file = open('OnlineNewsPopularity.csv')\n",
    "csvreader = csv.reader(file)\n",
    "header = [] \n",
    "header = next(csvreader)\n",
    "header = header[2:13] + header[39:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(target):\n",
    "    target = np.where(target < 6000, target, 6000)\n",
    "    target = np.where(target < 2000, -1, target)\n",
    "    target = np.where(target > 2000, 1, target)\n",
    "    \n",
    "    \n",
    "    return target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (1000, 29)\n",
      "Validation shape: (1000, 29)\n",
      "Test shape: (1000, 29)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(data)\n",
    "np.random.shuffle(data)\n",
    "num_training, num_validation, num_test = 1000, 2000, 3000\n",
    "\n",
    "\n",
    "training = data[:num_training, :]\n",
    "validation = data[num_training:num_validation, :]\n",
    "test = data[num_validation:num_test, :]\n",
    "training_target, validation_target, test_target = training[:, -1], validation[:, -1], test[:, -1]\n",
    "training = training[:, :-1]\n",
    "validation = validation[:, :-1]\n",
    "test = test[:, :-1]\n",
    "\n",
    "training_target, validation_target, test_target = get_target(training_target), get_target(validation_target), get_target(test_target)\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(training)\n",
    "training = scaler.transform(training)\n",
    "validation = scaler.transform(validation)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print(\"Training shape:\", training.shape)\n",
    "print(\"Validation shape:\", validation.shape)\n",
    "print(\"Test shape:\", test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(data, y, K, C, epsilon):\n",
    "    K += 1\n",
    "    eta = 1/K.diagonal()\n",
    "    \n",
    "    t = 0\n",
    "    alpha = np.zeros((data.shape[0],))\n",
    "    alpha_prev = alpha\n",
    "    \n",
    "    dif = 2*epsilon\n",
    "    while dif > epsilon: \n",
    "        for k in range(data.shape[0]):\n",
    "            temp = np.multiply(np.multiply(alpha, y), K[:,0])\n",
    "            alpha[k] = alpha[k] + eta[k]*(1 - y[k]*np.sum(temp))\n",
    "            \n",
    "            if alpha[k] < 0: alpha[k] = 0 \n",
    "            if alpha[k] > C: alpha[k] = C\n",
    "            \n",
    "        dif = np.linalg.norm(alpha-alpha_prev)\n",
    "        alpha_prev = alpha\n",
    "    \n",
    "    w = np.matmul(np.multiply(alpha, y), data)\n",
    "    return w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(data, y, w): \n",
    "    w = np.reshape(w, (w.shape[0], 1))\n",
    "    y_hat = np.matmul(data, w)\n",
    "    y_hat = np.where(y_hat < 0, -1, 1)\n",
    "    y_hat = np.reshape(y_hat, (data.shape[0],))\n",
    "    accuracy = 1 - (data.shape[0] - np.count_nonzero(y_hat-y))/data.shape[0]\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Kernal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear kernal results:\n",
      "Accuracy: 0.529\n",
      "Optimal C is: 0.001\n",
      "Optimal epsilon is: 0.1\n"
     ]
    }
   ],
   "source": [
    "C = [1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "epsilon = [1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "linear_K = np.matmul(training, training.T) + 1 \n",
    "\n",
    "opt_accuracy, opt_C, opt_eps, opt_w = 0, 0, 0, np.zeros((training.shape[1],))\n",
    "\n",
    "for c_val in C: \n",
    "    for eps_val in epsilon: \n",
    "        w = SVM(training, training_target, linear_K, c_val, eps_val)\n",
    "\n",
    "        accuracy = find_accuracy(validation, validation_target, w)\n",
    "        \n",
    "        if accuracy > opt_accuracy: \n",
    "            opt_accuracy, opt_C, opt_eps, opt_w = accuracy, c_val, eps_val, w\n",
    "\n",
    "print(\"Linear kernal results:\")\n",
    "print(\"Accuracy:\", round(find_accuracy(test, test_target, opt_w), 3))\n",
    "print(\"Optimal C is:\", opt_C)\n",
    "print(\"Optimal epsilon is:\", opt_eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(eigvalues, eigvectors, K_bar):\n",
    "    alpha = 0.95 \n",
    "    f_r = 0 \n",
    "    r = 1\n",
    "    while f_r < alpha: \n",
    "        f_r = np.sum(eigvalues[:r])/np.sum(eigvalues)\n",
    "        r += 1\n",
    "\n",
    "    C_r = eigvectors[:, :r]\n",
    "    A = np.matmul(K_bar, C_r)\n",
    "    \n",
    "    return f_r, r, C_r, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_kernal(K):\n",
    "    # Center the Kernal Matrix \n",
    "    num_points = K.shape[0]\n",
    "    I = np.identity(num_points)\n",
    "    ones = np.full((num_points, num_points), 1/num_points, dtype=float)\n",
    "\n",
    "    # Check that k_bar is found correctly \n",
    "    K_bar = np.matmul(np.matmul(I-ones, K), I-ones)\n",
    "\n",
    "    eigvalues, eigvectors = np.linalg.eigh(K_bar)\n",
    "    # Reverse the eigenvalues and eigenvectors to be in increasing order\n",
    "\n",
    "    eigvalues = np.flip(eigvalues, axis=0)\n",
    "    eigvectors = np.flip(eigvectors, axis=1)\n",
    "\n",
    "    variances = eigvalues/num_points \n",
    "    '''Some of the eigvalues are currently negative, we need to figure out what is going wrong'''\n",
    "    # Before finding ci ensure that ui = 1\n",
    "    eigvalues = np.where(eigvalues > 0, eigvalues, 1E-9)\n",
    "    #print(np.sqrt(1/eigvalues))\n",
    "    ci = np.sqrt(1/eigvalues)*eigvectors\n",
    "    \n",
    "    f_r, r, C_r, A = reduce_dim(eigvalues, ci, K_bar)\n",
    "    \n",
    "    return f_r, r, C_r, A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal sigma is:\n",
      "400008\n"
     ]
    }
   ],
   "source": [
    "mu = np.sum(training, axis=0)/num_points\n",
    "S = np.sqrt(np.sum(training**2, axis=1))\n",
    "sigma = 400000\n",
    "f_r_max = 0\n",
    "sigma_optimal = 0\n",
    "# 40000 to 40010 \n",
    "# Loop over values of sigma to find the optimal sigma value\n",
    "while sigma < 400010:  \n",
    "    gaussian_kernal = np.exp((2*np.dot(training, training.T)-S-S.T)/(2*sigma))\n",
    "\n",
    "    f_r, r, C_r, A = center_kernal(gaussian_kernal)\n",
    "    \n",
    "    if f_r > f_r_max: \n",
    "        sigma_optimal = sigma\n",
    "        f_r_max = f_r\n",
    "    \n",
    "    sigma += 2\n",
    "\n",
    "print(\"The optimal sigma is:\")\n",
    "print(sigma_optimal)\n",
    "\n",
    "gaussian_kernal = np.exp((2*np.dot(training, training.T)-S-S.T)/(2*sigma_optimal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian kernal results:\n",
      "Accuracy: 0.483\n",
      "Optimal C is: 1000\n",
      "Optimal epsilon is: 1\n"
     ]
    }
   ],
   "source": [
    "C = [1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "epsilon = [1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "opt_accuracy, opt_C, opt_eps, opt_w = 0, 0, 0, np.zeros((training.shape[1],))\n",
    "\n",
    "for c_val in C: \n",
    "    for eps_val in epsilon: \n",
    "        w = SVM(training, training_target, gaussian_kernal, c_val, eps_val)\n",
    "\n",
    "        accuracy = find_accuracy(validation, validation_target, w)\n",
    "        \n",
    "        if accuracy > opt_accuracy: \n",
    "            opt_accuracy, opt_C, opt_eps, opt_w = accuracy, c_val, eps_val, w\n",
    "\n",
    "print(\"Gaussian kernal results:\")\n",
    "print(\"Accuracy:\", round(find_accuracy(test, test_target, opt_w), 3))\n",
    "print(\"Optimal C is:\", opt_C)\n",
    "print(\"Optimal epsilon is:\", opt_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCI 6390\n",
    "Implement the inhomogeneous polynomial kernel. You need to select the best degree and CC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inhomogeneous Polynomial kernal results:\n",
      "Accuracy: 0.533\n",
      "Optimal C is: 1000\n",
      "Optimal epsilon is: 0.1\n",
      "Optimal Constant C is: 300\n",
      "Optimal degree is: 1\n"
     ]
    }
   ],
   "source": [
    "C = [1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "epsilon = [1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "opt_accuracy, opt_C, opt_eps, opt_w, opt_degree, opt_constant = 0, 0, 0, np.zeros((training.shape[1],)), 0, 0\n",
    "# C is a hyperparameter, so we must loop to find the optimal value \n",
    "\n",
    "C_max = 380\n",
    "C_kernal = 290\n",
    "\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5, 6]\n",
    "for i in degrees: \n",
    "    while C_kernal < C_max:\n",
    "        poly_kernal = (C_kernal+np.matmul(training, training.T))**i\n",
    "        \n",
    "        for c_val in C: \n",
    "            for eps_val in epsilon: \n",
    "                w = SVM(training, training_target, poly_kernal, c_val, eps_val)\n",
    "\n",
    "                accuracy = find_accuracy(validation, validation_target, w)\n",
    "                \n",
    "                if accuracy > opt_accuracy: \n",
    "                    opt_accuracy, opt_C, opt_eps, opt_w, opt_degree, opt_constant = accuracy, c_val, eps_val, w, i, C_kernal\n",
    "                \n",
    "        C_kernal += 10\n",
    "\n",
    "optimal_kernal = (opt_constant+np.matmul(training, training.T))**opt_degree\n",
    "\n",
    "print(\"Inhomogeneous Polynomial kernal results:\")\n",
    "print(\"Accuracy:\", round(find_accuracy(test, test_target, opt_w), 3))\n",
    "print(\"Optimal C is:\", opt_C)\n",
    "print(\"Optimal epsilon is:\", opt_eps)\n",
    "print(\"Optimal Constant C is:\", opt_constant)\n",
    "print(\"Optimal degree is:\", opt_degree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Exam I Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data matrix is:  (210, 8)\n",
      "The number of attributes is: 8\n",
      "The number of points is: 210\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('./seeds_dataset.txt')\n",
    "num_points = data.shape[0]\n",
    "num_attributes = data.shape[1]\n",
    "\n",
    "print(\"The shape of the data matrix is: \", data.shape)\n",
    "print(\"The number of attributes is:\", num_attributes)\n",
    "print(\"The number of points is:\",num_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5 (20 points) Non-linear direction in input space: For the polynomial kernel it is actually possible to compute the direction of the kernel principal component. For the homogeneous quadratic kernel, compute the best quadratic PC direction equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction of Kernal Principal Component:\n",
      "[0.06648823 0.06243041 0.05948238 0.05703907 0.06970657 0.06029944\n",
      " 0.06392164 0.05922553 0.07555933 0.07321256 0.0685746  0.05827729\n",
      " 0.05882851 0.05805518 0.05762364 0.06251495 0.05935507 0.06688338\n",
      " 0.06038418 0.05312137 0.06115463 0.06006921 0.06761577 0.04767448\n",
      " 0.06455941 0.07044301 0.0542909  0.05235173 0.05968839 0.05731687\n",
      " 0.05330617 0.06885045 0.06192749 0.05848585 0.06515699 0.07086492\n",
      " 0.07297094 0.07726552 0.06430687 0.06342393 0.05626048 0.05584997\n",
      " 0.05313454 0.07036498 0.06543768 0.05675541 0.06570347 0.06481492\n",
      " 0.06350824 0.06428204 0.0628991  0.07166263 0.06436914 0.06149572\n",
      " 0.06235006 0.06522933 0.06152957 0.06220119 0.06645448 0.04820491\n",
      " 0.04481283 0.04323787 0.04946429 0.05624041 0.05093028 0.05166097\n",
      " 0.0602224  0.05941395 0.06060974 0.05358426 0.08489106 0.08018873\n",
      " 0.08202724 0.09180729 0.07880803 0.07965939 0.08246567 0.10644735\n",
      " 0.09487004 0.07881178 0.07820644 0.09157004 0.10232914 0.09498377\n",
      " 0.09599312 0.08678976 0.08923342 0.09477253 0.10999511 0.10685931\n",
      " 0.099692   0.09002286 0.09081015 0.09113666 0.09255881 0.07962539\n",
      " 0.09514122 0.09243012 0.08760374 0.08952157 0.07555544 0.08330421\n",
      " 0.09564758 0.09449511 0.09229902 0.08963722 0.09022649 0.08364373\n",
      " 0.10032074 0.08738159 0.08688536 0.09571541 0.09103773 0.09730928\n",
      " 0.1079896  0.0920386  0.0915898  0.09315694 0.09119221 0.09985876\n",
      " 0.10355876 0.08696552 0.07519921 0.08689733 0.07054822 0.09085859\n",
      " 0.09178084 0.08366189 0.09954131 0.08340465 0.0859208  0.09094094\n",
      " 0.07056245 0.07520972 0.07174199 0.06800359 0.08210025 0.07073906\n",
      " 0.07052998 0.07474816 0.05819469 0.06082735 0.05969151 0.05280431\n",
      " 0.05084258 0.04925882 0.04676773 0.05315295 0.05426319 0.04666555\n",
      " 0.05103249 0.05407225 0.05358295 0.04638998 0.0478216  0.04856833\n",
      " 0.04631965 0.05335621 0.05115669 0.04995496 0.05370596 0.05136561\n",
      " 0.05189106 0.05400881 0.04764301 0.04899386 0.05413524 0.05183639\n",
      " 0.04800983 0.04656316 0.04859164 0.05032305 0.04708218 0.04896118\n",
      " 0.04661828 0.04470191 0.04811926 0.0448907  0.04930065 0.0503608\n",
      " 0.0477715  0.05350626 0.05225306 0.04955488 0.05772021 0.04937439\n",
      " 0.05157937 0.04520902 0.04889187 0.04330769 0.0462422  0.04654345\n",
      " 0.04900895 0.04572507 0.05100296 0.05449617 0.05522648 0.05780692\n",
      " 0.05381805 0.05226982 0.05354387 0.05113958 0.04559752 0.05694477\n",
      " 0.05229498 0.05038497 0.04677829 0.05972707 0.04936608 0.05311539]\n"
     ]
    }
   ],
   "source": [
    "# We know any direction is a linear combination of points \n",
    "quadratic_kernal = (np.matmul(data, data.T))**2\n",
    "f_r, r, C_r, A = center_kernal(quadratic_kernal)\n",
    "\n",
    "np.random.seed(42)\n",
    "eps = 1e-4\n",
    "\n",
    "# Initialize X0\n",
    "Xprev = np.random.rand(num_points, 2) \n",
    "Xprev[:,0], Xprev[:,1] = Xprev[:,0]/np.linalg.norm(Xprev[:,0]), Xprev[:,1]/np.linalg.norm(Xprev[:,1])\n",
    "\n",
    "eiga, eigb = 0, 0 \n",
    "\n",
    "# Find the convergence \n",
    "dif = 2*eps\n",
    "while(dif > eps): \n",
    "    Xi = np.matmul(quadratic_kernal, Xprev)\n",
    "\n",
    "    # Orthogonalize the columns \n",
    "    a, b = np.reshape(Xi[:,0], (num_points, 1)), np.reshape(Xi[:,1], (num_points, 1))\n",
    "    b = b - (np.matmul(b.T, a)/np.dot(a.T, a)*a)\n",
    "\n",
    "    # Normalize the columns \n",
    "    a, b = a/np.linalg.norm(a), b/np.linalg.norm(b)\n",
    "    Xi[:, 0], Xi[:, 1]= np.reshape(a, (num_points,)), np.reshape(b, (num_points,))\n",
    "\n",
    "    dif = np.linalg.norm(Xprev-Xi)\n",
    "    Xprev = Xi \n",
    "\n",
    "eigvectors_new = Xprev\n",
    "\n",
    "# Find the eigenvalues (there is no need to divide by length of eigenvector since it is normalized)\n",
    "eiga = np.dot(np.matmul(quadratic_kernal, eigvectors_new[:, 0]), eigvectors_new[:, 0])  \n",
    "eigb = np.dot(np.matmul(quadratic_kernal, eigvectors_new[:, 1]), eigvectors_new[:, 1])  \n",
    "\n",
    "print(\"Direction of Kernal Principal Component:\")\n",
    "print(eigvectors_new[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8 (**CSCI 6390 Only**: 10 points) Let $\\mathbf{a}$ be random $d$-dimensional binary vector, and $\\mathbf{b}$ a random $d$-dimensional corner in a $d$-dimensional unit hypercube (with the range in each axis as $[0,1]$). Derive the formula for the expected angle between $\\mathbf{a}$ and $\\mathbf{b}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that a is a d-dimensional binary vector and lets assume that b is a random d-dimensional coordinates of a corner in a d-dimensional hyper cube. By placing the origin at the center of the cube, as well as the a vector start point we find that the resulting expected angle between a and b will be $\\theta = cos^{-1}\\frac{ab}{|a||b|}$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99ef57a3e309b3ccef27bfdc21155d00f14e44d33227cb907457916f15963b11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
